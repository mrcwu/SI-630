{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0818d4b1-fb69-4a10-bc64-cbe172868a2e",
   "metadata": {},
   "source": [
    "#### install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ccdff0-b535-4652-a7c8-4de92147f6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622f8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5620e31-6d60-44d3-b8a1-9ac9e8a9f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "100b02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d93d7a-496e-44b8-b3e3-3a38a0c14e54",
   "metadata": {},
   "source": [
    "### Problem 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6ff2-28fa-4439-923f-89d5a8d05d64",
   "metadata": {},
   "source": [
    "#### 13.1 import packages, raw data, models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e2b41f-8810-41f2-8c6e-9bd84d67c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d66fcdd-0c77-4f36-b546-13efb6344f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df87745b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978916c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281e084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8162be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d5ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf0d293-1ae9-496f-9993-a078a67c7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = pd.read_csv('si630w22-hw3-train.csv')\n",
    "test_data = pd.read_csv('si630w22-hw3-test.public.csv')\n",
    "text_data = pd.read_csv('si630w22-hw3-data.csv')\n",
    "dev_data = pd.read_csv('si630w22-hw3-dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a78bab83-730d-4a50-962c-63a900d01358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the tokenizer\n",
    "# Call the pretrained model\n",
    "model_ckpt = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606461d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification as AMFSC\n",
    "model = AMFSC.from_pretrained(model_ckpt, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827d4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    mse = mean_absolute_error(labels, preds)\n",
    "    return {'mse': mse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ef084",
   "metadata": {},
   "source": [
    "#### 13.2 Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40114079-f23d-4d04-a5a6-6f22f85f3ea2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a717402a7146fa89766cbd60273a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 307\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>18.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>17.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>16.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>14.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>13.394900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91566a6ec83c41c18170981dd3378361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 310\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>9.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>8.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>8.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.674600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfff932d8f94fbdbba58e8f7f6fdf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 302\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.703100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c839076b0a7c4b3c8d7fe7bd74dd8fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 303\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.854000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f59415160e45128d52cf42558c4527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 311\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.862700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.519100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e146ae4e20404ba7bf9aea1ecab005ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 293\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.463600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce21d37d12764eaf9b8fe6f33fbf0bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 288\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-9\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-9/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-9/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-9/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-9/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-18\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-18/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-18/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-18/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-18/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-27\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-27/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-27/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-27/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-27/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-36\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-36/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-36/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-36/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-36/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-45\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-45/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-45/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392f7b7257e14363bf89c12683017453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 294\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413678f297ea423cb4d027a50afdbbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 304\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cc14ba7eb6467ca19e3fd0bb3a835f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 298\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.510500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cd0d80824a4af6b070c530b57aea1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 317\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e9afdee79747529c624cc428418f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 294\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.528200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867b14f4ff5f4ea8be4052e4a96cc34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 315\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64a4c16c3c848e784b365a84a0eb66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 313\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5f34fc58a54f29be02b5b282b03b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 314\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc2a7d685aa44c39e8f3edac47541f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 302\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1e6f25317e44ac9e2f62ffb9c32913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 306\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000a871352a84313892785a9cb2d40e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 308\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.316300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32de8ecbd0ff4af8a17516bb997cb5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 301\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.738500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1552ac48a53445bba268eadfa5ca686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 295\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0038bd6732fa4703857f1dd8881338a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 304\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.508700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4046a0ba4d14ab5bc8002082f055ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 302\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.302100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e80f02592448bebd3949a40a689e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 283\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-9\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-9/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-9/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-9/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-9/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-18\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-18/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-18/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-18/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-18/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-27\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-27/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-27/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-27/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-27/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-36\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-36/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-36/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-36/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-36/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-45\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-45/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-45/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35fca3775c2418db13997a3d606883c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question_id, text. If question_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 298\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.327600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-10\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-10/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-10/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-20\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-20/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-20/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-30\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-30/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-40\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-40/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-40/special_tokens_map.json\n",
      "Saving model checkpoint to minln-finetuned-rating-regression/checkpoint-50\n",
      "Configuration saved in minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model weights saved in minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/conanwu/minln-finetuned-rating-regression/checkpoint-50\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/conanwu/minln-finetuned-rating-regression/checkpoint-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/added_tokens.json. We won't load it.\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/vocab.txt\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/special_tokens_map.json\n",
      "loading file /home/conanwu/minln-finetuned-rating-regression/checkpoint-50/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "text_data['text'] = text_data[['question_text', 'reply_text']].agg(' [SEP] '.join, axis=1)\n",
    "\n",
    "g_lst = []\n",
    "a_lst = []\n",
    "b_lst = []\n",
    "c_lst = []\n",
    "\n",
    "\n",
    "def load_data_to_dataset(text_df, df, is_test_data = False):\n",
    "\n",
    "    if is_test_data == True:\n",
    "        df_processed = df\n",
    "        merge_df = pd.merge(df_processed, text_df, left_on='id',right_on='question_id',how='inner')\n",
    "        data = merge_df[['question_id','text','rating']]\n",
    "        \n",
    "#         output = Dataset.from_pandas(data, preserve_index=False)\n",
    "        output = data\n",
    "        \n",
    "    else:\n",
    "        df_processed = df.groupby('id').mean('rating')\n",
    "        merge_df = pd.merge(df_processed, text_df, left_on='id',right_on='question_id',how='inner')\n",
    "\n",
    "        data = merge_df[['question_id','text','rating']]\n",
    "        data = data.rename(columns={'rating': 'labels'})\n",
    "        data['labels']= data['labels']\n",
    "\n",
    "        output = Dataset.from_pandas(data, preserve_index=False)\n",
    "\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "group_lst = dev_data['group'].unique()\n",
    "for group in group_lst:\n",
    "#     n = 0\n",
    "    print(group)\n",
    "    # a b c data definition\n",
    "    dev_data_a = dev_data.copy()\n",
    "    dev_data_a['rating'] = np.random.randint(1, 6, dev_data.shape[0])\n",
    "    grouped_a = dev_data_a.groupby(['group','id'])['rating'].mean()\n",
    "    grouped_a = grouped_a.to_frame()\n",
    "    grouped_a = grouped_a.reset_index()\n",
    "    grouped_a = grouped_a[grouped_a['group'] == group]\n",
    "    \n",
    "    a = grouped_a[['id','rating']].reset_index(drop=True)\n",
    "    \n",
    "#     group_b = dev_data.groupby(['group','id'])['rating'].mean().to_frame().reset_index()\n",
    "#     group_b = group_b[group_b['group'] == group][['id','rating']].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     b = group_b\n",
    "    b = dev_data[(dev_data['id'].isin(dev_data[dev_data['group'] == group].id.unique())) & (dev_data['group'] == group)].groupby('id').mean('rating').reset_index()\n",
    "    c = dev_data[(dev_data['id'].isin(dev_data[dev_data['group'] == group].id.unique())) & (dev_data['group'] != group)].groupby('id').mean('rating').reset_index()\n",
    "    \n",
    "    \n",
    "    # train data definition\n",
    "    train_group_data = train_data[train_data['group'] == group]\n",
    "    train_ds = load_data_to_dataset(text_data, train_group_data, is_test_data = False)\n",
    "    train_ds = train_ds.map(tokenize_text, batched=True)\n",
    "    \n",
    "    \n",
    "    # model args\n",
    "    batch_size = 32\n",
    "    logging_steps = len(train_ds) // batch_size\n",
    "    output_dir = 'minln-finetuned-rating-regression'\n",
    "    training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                      num_train_epochs=5,\n",
    "                                      learning_rate=2e-5,\n",
    "                                      per_device_train_batch_size=batch_size,\n",
    "                                      per_device_eval_batch_size=batch_size,\n",
    "    #                                   evaluation_strategy='epoch',\n",
    "                                      save_strategy = 'epoch',\n",
    "                                      logging_steps=logging_steps,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      push_to_hub=False,\n",
    "                                      fp16=True\n",
    "                                     )\n",
    "\n",
    "    \n",
    "    # model parameter\n",
    "    trainer = Trainer(model = model, \n",
    "                      args = training_args, \n",
    "                      train_dataset = train_ds, \n",
    "#                       eval_dataset = None,\n",
    "                      compute_metrics = compute_metrics,\n",
    "                      tokenizer = tokenizer)\n",
    "    # training model\n",
    "    trainer.train()\n",
    "    \n",
    "    location = '/home/conanwu/minln-finetuned-rating-regression/checkpoint-50'\n",
    "    pipe = pipeline(\"text-classification\", model=location, function_to_apply=\"none\", truncation=True, max_length=512)\n",
    "    \n",
    "    g_lst.append(group)\n",
    "    \n",
    "    # testing model\n",
    "    i_count = 0\n",
    "    for i in a,b,c:\n",
    "        test_group_data = i\n",
    "\n",
    "        test_ds = load_data_to_dataset(text_data, test_group_data, is_test_data = True)\n",
    "        \n",
    "\n",
    "        def return_score(text):\n",
    "            return pipe(text)[0]['score']\n",
    "        \n",
    "        \n",
    "        test_ds['predicted'] = test_ds['text'].apply(return_score)\n",
    "        \n",
    "        corr_num = test_ds.corr(method='pearson').iloc[1][0]\n",
    "        if i_count == 0:\n",
    "            a_lst.append(corr_num)\n",
    "        elif i_count == 1:\n",
    "            b_lst.append(corr_num)\n",
    "        elif i_count == 2:\n",
    "            c_lst.append(corr_num)\n",
    "        \n",
    "        i_count += 1\n",
    "        \n",
    "#     n += 1\n",
    "#     if n == 1:\n",
    "#         break\n",
    "\n",
    "# save the result\n",
    "output_p13_dict = {'group':g_lst, 'a':a_lst, 'b':b_lst, 'c':c_lst}\n",
    "output_p13_df = pd.DataFrame(output_p13_dict)\n",
    "\n",
    "output_p13_df.to_csv('output_p13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bdf4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_p13_df = output_p13_df.sort_values('group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f527ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the result\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')    \n",
    "f, ax = plt.subplots(figsize=(16,8))\n",
    "output_p13_df.plot(kind='bar', x='group', sharex=True, ax=ax, rot=0, color=['b','r','g'], width=0.8)\n",
    "ax.set_title(\"Group and abc annotation\")\n",
    "ax.set_xlabel(\"Group\")\n",
    "ax.grid(axis=\"y\", color=\"grey\")\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce0817-be87-4c60-aaf8-a33421680e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
